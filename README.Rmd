---
title: "Simulations for Conformal Inference in the probably Package"
date: "2023-06-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    comment = "#>",
    fig.align = 'center',
    fig.path = "figures/",
    fig.width = 5,
    fig.height = 6,
    out.width = "75%",
    dev = 'svg',
    dev.args = list(bg = "transparent"),
    tidy = FALSE,
    echo = FALSE
  )

# ------------------------------------------------------------------------------

library(tidymodels)
library(probably)

# ------------------------------------------------------------------------------

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)

# ------------------------------------------------------------------------------

rs_cols <- RColorBrewer::brewer.pal(9, "YlOrRd")[-(1:2)]
mth_cols <- RColorBrewer::brewer.pal(3, "Dark2")
mth_lvl <- c("split", "quantile", "cv+")

# ------------------------------------------------------------------------------

load("simulation_results.RData")
load("cv_results.RData")

# The "overfit nnet" model didn't exactly overfit as much as thought
# so we'll remove it to reduce redundancy 

basic_coverage     <- filter(basic_coverage, model != "nnet (overfit)")
cv_results         <- filter(cv_results, model != "nnet (overfit)")
resampled_coverage <- filter(resampled_coverage, model != "nnet (overfit)")

# ------------------------------------------------------------------------------

lvls <- 
  unique(basic_coverage$conf_level * 100) %>% 
  paste0("%") %>% 
  knitr::combine_words()

training <- 
  unique(basic_coverage$training_size) %>% 
  sort() %>% 
  format(big.mark = ",") %>% 
  knitr::combine_words()

rmse_tab <- 
  cv_results %>% 
  filter(metric == "rmse") %>% 
  select(-metric) %>% 
  arrange(mean) %>% 
  rename(RMSE = mean)

rsq_tab <- 
  cv_results %>% 
  filter(metric == "rsq") %>% 
  select(-metric) %>% 
  arrange(desc(mean)) %>% 
  rename(`R<sup>2</sup>` = mean)

```

The `probably` package has several functions that can produce prediction intervals using conformal inference. This repo has some simulations to evaluate coverage properties for our implementations. 

## Simulation Conditions

The factors in the simulations

 - Confidence level: `r lvls`.
 - Size of the training sets:  `r training`.
 - The type and amount of resampling (CV+ method only, see below). 
 - The model type: 
   - CART trees (default parameters),
   - Cubist rules (20 committees and 7 nearest neighbors), 
   - Linear regression via ordinary least squares and simple main effects, and 
   - Single layer, feed-forward neural networks (7 hidden units) and was trained for 100 epochs. 

Each combination of settings was repeated 25 times. 

The data were simulated using the method detailed in Hooker (2004) and Sorokina _et al._ (2008) with a prediction function of 

```{r}
#| echo: false

knitr::include_graphics("figures/equation.png")
```

Predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on `[0.6, 1.0]`. The errors are normal with mean zero and default standard deviation of 0.25. Note that this method simulates a constant error term (e.g., homoscedasticity), which will come into play when quantile regression is used. 

The coverage properties were assessed using a test set of 1,000 simulated data points. The "mean coverage" statistics shown below are the averages of 25 coverage statistics (each of which is based on 1,000 samples).

The calibration data set size was held constant at 500 samples for the two split methods discussed below. 

In terms of model performance, measured via 10-fold cross-validation, showed that two models had very good results, one was mediocore, and another signficanty underfits the data: 

```{r}
#| label: rmse-tbl
#| results: 'asis'

knitr::kable(rmse_tab, digits = 3)
```

The best possible root mean squared error was 0.25.

The coefficients of determination were: 

```{r}
#| label: rsq-tbl
#| results: 'asis'

knitr::kable(rsq_tab, digits = 3)
```

Each section below evaluated the coverage properties for each conformal method.

## Split Conformal

The coverage results from using `int_conformal_split()` were: 

```{r}
#| label: split-coverage
#| fig.align: 'center'
#| fig.height: 4

basic_coverage %>% 
  filter(method == "split") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

We can also compare the conformal intervals for those produced naturally when ordinary least squares (OLS) is used to fit the model (with the usual normality assumptions on the residuals). We can assess both the difference in coverage and interval width. The simple differences in coverage were: 

```{r}
#| label: split-coverage-lm
#| fig.height: 4
#| fig.width: 6
#| fig.align: 'center'
#| out.width: "70%"

lm_comparison %>% 
  filter(method == "split") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Training Set Size")
```

While the coverage for 90% intervals was lower for this conformal method, the scale of the y-axis indicates that it is a meager difference.

The percent differences in the widths of the intervals were:

```{r}
#| label: split-width-lm
#| fig.height: 4
#| fig.width: 6
#| fig.align: 'center'
#| out.width: "70%"

lm_comparison %>% 
  filter(method == "split") %>% 
  ggplot(aes(x = training_size, y = width)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Training Set Size")
```

We might expect the parametric intervals for be more narrow. The 90% intervals show the opposite but this is a 2% difference (at most). 

## Split Quantile Conformal

`int_conformal_quantile()` uses quantile random forests as the quantile regression method. Its coverage results are: 

```{r}
#| label: quantile-coverage
#| fig.align: 'center'
#| fig.height: 4

basic_coverage %>% 
  filter(method == "quantile") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

The smallest training set result appears to have slightly high coverage. Recall that the simulation has constant variance and this method is best for cases where the variance changes over different conditions. 

To again compare this method to the parametric OLS interval methods, the simple differences in coverage show definite systematic trends that show the coverage changes as the training set size changes. 

```{r}
#| label: quantile-coverage-lm
#| fig.height: 4
#| fig.width: 6
#| fig.align: 'center'
#| out.width: "70%"

lm_comparison %>% 
  filter(method == "quantile") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Training Set Size")
```

The percent differences in the widths of the intervals were:

```{r}
#| label: quantile-width-lm
#| fig.height: 4
#| fig.width: 6
#| fig.align: 'center'
#| out.width: "70%"

lm_comparison %>% 
  filter(method == "quantile") %>% 
  ggplot(aes(x = training_size, y = width)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Training Set Size")
```

This is not too surprising; the method is not consistent with how these data are generated. In a way, this might be the worst-case result. 


## CV+

For cross-validation, the simulations focused on V-fold cross-validation with V = 10. For this method, using `int_conformal_cv()`, the coverage results are not great with smaller training set sizes and _some_ models:

```{r}
#| label: cv-coverage
#| fig.align: 'center'
#| fig.height: 4

basic_coverage %>% 
  filter(method == "cv+" & resamples == 10) %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

This _may_ be related to how this method centers the intervals: the interval center is the average of the predictions from the 10 held-out models. It could be that, for some models, the average held-out new prediction is not centered in the right place. 

When compared with the OLS intervals, the results are interesting: 

```{r}
#| label: cv-coverage-lm
#| fig.height: 4
#| fig.width: 6
#| fig.align: 'center'
#| out.width: "70%"

lm_comparison %>% 
  filter(method == "cv+" & resamples == 10) %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Training Set Size")
```

The percent differences in the widths of the intervals were:

```{r}
#| label: cv-width-lm
#| fig.height: 4
#| fig.width: 6
#| fig.align: 'center'
#| out.width: "70%"

lm_comparison %>% 
  filter(method == "cv+" & resamples == 10) %>% 
  ggplot(aes(x = training_size, y = width)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(x = "Training Set Size")
```

For OLS models, the results look pretty good. This is an underfit model though; perhaps the issue is related to model complexity. 

## More Resamples

The methodology has only been derived for simple cross-validation. How does it work when other methods are used? For example, we could also use repeated 10-fold cross-validation as well as another method such as the bootstrap. 

For repeated cross-validation, we can look at different amounts of resampling (from two to five repeats). The coverage: 

```{r}
#| label: cv-resamples-coverage
#| fig.align: 'center'
#| fig.height: 4

resampled_coverage %>% 
  filter(method == "cv+" & resampling == "Cross-Validation") %>% 
  mutate(resamples = format(resamples)) %>% 
  ggplot(aes(x = training_size, y = coverage, 
             col = resamples, group = resamples)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  geom_line() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

Again, the results are model-dependent, with the worst results being the  neural network. Coverage convergences, for this simulation, around 5K or 10K training set samples. 

What if the bootstrap is used to resample the model, using varying amounts of resamples? The results are: 

```{r}
#| label: boot-resamples-coverage
#| fig.align: 'center'
#| fig.height: 4

resampled_coverage %>% 
  filter(method == "cv+" & resampling == "Bootstrap") %>% 
  mutate(resamples = format(resamples)) %>% 
  ggplot(aes(x = training_size, y = coverage, 
             col = resamples, group = resamples)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  geom_line() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

These look a lot like the cross-validation results.

More resamples certainly improves coverage and the bootstrap appears to have slightly better coverage as 10-fold cross-validation. 

## References

