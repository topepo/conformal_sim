---
title: "Simulations for Conformal Inference in the probably Package"
date: "2023-06-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    comment = "#>",
    fig.align = 'center',
    fig.path = "figures/",
    fig.width = 7,
    fig.height = 7,
    out.width = "75%",
    dev = 'svg',
    dev.args = list(bg = "transparent"),
    tidy = FALSE,
    echo = FALSE
  )

# ------------------------------------------------------------------------------

library(tidymodels)

# ------------------------------------------------------------------------------

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)

# ------------------------------------------------------------------------------

rs_cols <- RColorBrewer::brewer.pal(9, "YlOrRd")[-(1:2)]
mth_cols <- RColorBrewer::brewer.pal(3, "Dark2")
mth_lvl <- c("split", "quantile", "cv+")

# ------------------------------------------------------------------------------

load("simulation_results.RData")

lvls <- 
  unique(basic_coverage$conf_level * 100) %>% 
  paste0("%") %>% 
  knitr::combine_words()

training <- 
  unique(basic_coverage$training_size) %>% 
  sort() %>% 
  format(big.mark = ",") %>% 
  knitr::combine_words()

```

The `probably` package has several functions that can produce prediction intervals using conformal inference. This repo has some simulations to evaluate coverage properties for our implementations. 

The factors in the simulations

 - Confidence level: `r lvls`.
 - Size of the training sets:  `r training`.
 - The type and amount of resampling (CV+ method only, see below). 
 - The model type: 
   - CART trees (default parameters),
   - Cubist rules (20 committees and 7 nearest neighbors), 
   - Linear regression via ordinary least squares and simple main effects, and 
   - Single layer, feed-forward neural networks (one properly tuned with 7 hidden units and another that deliberately overfits with 50 hidden units). Both were trained for 100 epochs. 

Each combination of settings was repeated 25 times. 

The data were simulated using the method detailed in Hooker (2004) and Sorokina _et al._ (2008) with a prediction function of 

```{r}
#| echo: false

knitr::include_graphics("figures/equation.png")
```

Predictors 1, 2, 3, 6, 7, and 9 are standard uniform while the others are uniform on `[0.6, 1.0]`. The errors are normal with mean zero and default standard deviation of 0.25. Note that this method simulates a constant error term (e.g., homoscedasticity), which will come into play when quantile regression is used. 

The coverage properties were assessed using a test set of 1,000 simulated data points. The "mean coverage" statistics shown below are the averages of 25 coverage statistics (each of which is based on 1,000 samples).

The calibration data set size was held constant at 500 samples for the two split methods discussed below. 

Each section below evaluated the coverage properties for each conformal method.

## Split Conformal

The coverage results from using `int_conformal_split()` were: 

```{r}
#| label: split-coverage
#| 

basic_coverage %>% 
  filter(method == "split") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

We can also compare the conformal intervals for those produced naturally when ordinary least squares (OLS) is used to fit the model (with the usual normality assumptions on the residuals). We can assess both the difference in coverage and interval width. The simple differences in coverage were: 

```{r}
#| label: split-coverage-lm
#| fig.height: 4
#| fig.width: 7
#| out.width: "55%"

lm_comparison %>% 
  filter(method == "split") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(y = "Conformal - Parametric", title = "coverage") +
  labs(x = "Training Set Size")
```

While the coverage for 90% intervals was lower for this conformal method, the scale of the y-axis indicates that it is a meager difference.

The percent differences in the widths of the intervals were:

```{r}
#| label: split-width-lm
#| fig.height: 4
#| fig.width: 7
#| out.width: "55%"

lm_comparison %>% 
  filter(method == "split") %>% 
  ggplot(aes(x = training_size, y = width)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(y = "conformal - parametric", title = "coverage") +
  labs(x = "Training Set Size")
```

We might expect the parametric intervals for be more narrow. The 90% intervals show the opposite but this is a 2% difference (at most). 

## Split Quantile Conformal

`int_conformal_quantile()` uses quantile random forests as the quantile regression method. Its coverage results are: 

```{r}
#| label: quantile-coverage
#| 

basic_coverage %>% 
  filter(method == "quantile") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(aes(yintercept = conf_level), lty = 3) +
  geom_point() +
  facet_grid(model ~ conf_level) +
  labs(x = "Training Set Size", y = "Mean Coverage")
```

The smallest training set result appears to have slightly high coverage. Recall that the simulation has constant variance and this method is best for cases where the variance changes over different conditions. 

To again compare this method to the parametric OLS interval methods, the simple differences in coverage show definite systematic trends that show the coverage changes as the training set size changes. 

```{r}
#| label: quantile-coverage-lm
#| fig.height: 4
#| fig.width: 7
#| out.width: "55%"

lm_comparison %>% 
  filter(method == "quantile") %>% 
  ggplot(aes(x = training_size, y = coverage)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(y = "Conformal - Parametric", title = "coverage") +
  labs(x = "Training Set Size")
```

The percent differences in the widths of the intervals were:

```{r}
#| label: quantile-width-lm
#| fig.height: 4
#| fig.width: 7
#| out.width: "55%"

lm_comparison %>% 
  filter(method == "quantile") %>% 
  ggplot(aes(x = training_size, y = width)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_point() +
  facet_wrap( ~ conf_level) +
  scale_y_continuous(labels = scales::percent) + 
  labs(y = "conformal - parametric", title = "coverage") +
  labs(x = "Training Set Size")
```

This is not too surprising; the method is not consistent with how these data are generated. In a way, this might be the worst-case result. 


## CV+

For cross-validation, the simulations focused on V-fold cross-validation with V = 10. 

These results show TODO when compared with the OLS intervals: 


The methodology has only been derived for simple cross-validation. How does it work when other methods are used? For example, we could also use repeated 10-fold cross-validation as well as another method such as the bootstrap. 

For repeated cross-validation, 

If the bootstrap is used to resample the model, using varying amounts of resamples, there results are: 



## References

